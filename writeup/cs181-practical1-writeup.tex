\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Declare commands
\newcommand{\mat}[1]{\mathbf{#1}}

\title{CS 181 -- Practical 1}
\author{Casey Grun, Sam Kim, Rhed Shi}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Warmup}

\section{Approaches considered}
K-Means, K-nearest neighbor, PCA, SVD, PMF

\section{Probabilistic Matrix Factorization}
\subsection{Derivation}
The probabilistic matrix factorization (PMF) approach \cite{Mnih:2007wg} attempts to decompose the ratings matrix $\mat{R}$ into two matrices: $\mat{P}$ representing the users and $\mat{Q}$ representing the books:
$$\mat{R} = \mat{P} \times \mat{Q}^T$$
where:
$$ 
\mat{P} = 
\begin{pmatrix}
	\vec{p_1} \\
	\vec{p_2} \\
	\vdots    \\
	\vec{p_N} 
\end{pmatrix} = 
\begin{pmatrix}
	p_{1,1} & p_{1,2} & \ldots & p_{1,K} \\
	p_{2,1} & p_{2,2} & \ldots & p_{2,K} \\
	\vdots  & \vdots  & \ddots & \vdots  \\
	p_{N,1} & p_{N,2} & \ldots & p_{N,K} \\
\end{pmatrix}
$$
$$ 
\mat{Q} = 
\begin{pmatrix}
	\vec{q_1} \\
	\vec{q_2} \\
	\vdots    \\
	\vec{q_N} 
\end{pmatrix} = 
\begin{pmatrix}
	q_{1,1} & q_{1,2} & \ldots & q_{1,K} \\
	q_{2,1} & q_{2,2} & \ldots & q_{2,K} \\
	\vdots  & \vdots  & \ddots & \vdots  \\
	q_{N,1} & q_{N,2} & \ldots & q_{N,K} \\
\end{pmatrix}
$$
Each entry $\vec{p_i}$ in $\mat{P}$ represents a vector of $K$ latent features of user $i$, while $\vec{q_j}$ represents the $k$ latent factors of the book. If we can produce the matrices $\mat{P}$ and $\mat{Q}$, we can predict any entry $r_{i,j}$ in $\mat{R}$ by taking inner products of the vectors $\vec{p_i}$ and $\vec{q_j}$:
$$\hat{r}_{i,j} = \vec{p_i} \cdot \vec{q_j}^T \approx r_{i,j}$$

However, we only know a subset of the full matrix $\{r_{i,j}\}_{i=1, j=1}^{N,D}$; call that subset $T = \{(i, j, r_{i,j})\}$. We therefore need to iteratively train $\mat{P}$ and $\mat{Q}$, based only on the data in $T$. 

We recognize that some aspects of the ratings for a particular book are not explained jointly by the user and book, but by either the user or the book alone; we follow the approach of Paterek, Koren and Bell, and others in calling these terms \emph{biases} \cite{Paterek:2007va,Koren:2009uc}. A bias is the component of a user's ratings (or a book's ratings) varying from the global mean, $\mu = \frac{1}{ND} \sum_{i=1}^{N}\sum_{j=1}^{D} r_{i,j}$. Let $\vec{b} = (b_1, b_2, \ldots b_N)$ represent the biases for each users and $\vec{c} = (c_1, c_2, \ldots c_D)$ represent the biases for each book. We will adopt a slightly more sophisticated model for predicting the ratings, such that:
$$\hat{r}_{i,j} = \mu + b_i + c_j + \vec{p_i} \cdot \vec{q_j}^T \approx r_{i,j}$$
or:
$$\mat{R} \approx \mat{P} \times \mat{Q}^T + (\mu + \vec{b} \otimes \vec{c})$$
where $\otimes$ is the outer product.

To learn the values of $\mat{P}$ and $\mat{Q}$, we will minimize an error function:
$$E = \sum_{i,j,r_{i,j} \in T} e_{i,j} $$
$$E = \sum_{i,j,r_{i,j} \in T} \left(r_{i,j} - \hat{r}_{i,j}\right)^2 + \frac{\beta}{2} \left( ||p_i||^2 + ||q_i||^2 + b_i^2 + b_j^2 \right) $$
The first term of this error function simply describes the squared reconstruction error. The second term enforces regularization---penalizing rows of $\mat{P}$ and $\mat{Q}$ that have high magnitude. 

From this expression, we can derive $\nabla e_{i,j}$; this will allow us to minimize $e_{i,j}$ (and hence $E$) by stochastic gradient descent.

\begin{align*}
\frac{\partial e_{i,j}}{\partial p_{i,k}} &= \frac{\partial}{\partial p_{i,k}} \left(r_{i,j} - \hat{r}_{i,j}\right)^2 + \frac{\partial}{\partial p_{i,k}} \frac{\beta}{2} \left( ||p_i||^2 + ||q_i||^2 + b_i^2 + b_j^2 \right) \\
    &= 2 \left(r_{i,j} - \hat{r}_{i,j}\right)  \left(-\frac{\partial}{\partial p_{i,k}} \hat{r}_{i,j}\right)  + \frac{\partial}{\partial p_{i,k}} \frac{\beta}{2} \left( ||p_i||^2\right) \\
    &= 2 \left(r_{i,j} - \hat{r}_{i,j}\right)  \left(-\frac{\partial}{\partial p_{i,k}} \left(\mu + b_i + c_j + \vec{p_i} \cdot \vec{q_j}^T \right) \right)  + \frac{\partial}{\partial p_{i,k}} \frac{\beta}{2} \left( ||p_i||^2 \right) \\
    &= 2 \left(r_{i,j} - \hat{r}_{i,j}\right)  \left(-\frac{\partial}{\partial p_{i,k}} \left( \sum_{k=1}^{K} p_{i,k} q_{j,k} \right) \right)  + \frac{\partial}{\partial p_{i,k}} \frac{\beta}{2} \left( \sum_{k=1}^{K} p_{i,k}^2 \right) \\
    &= -2 \left(r_{i,j} - \hat{r}_{i,j}\right)  \left(q_{j,k} \right) + \beta p_{i,k} \\
\frac{\partial e_{i,j}}{\partial q_{j,k}} &= -2 \left(r_{i,j} - \hat{r}_{i,j}\right)  \left(p_{i,k} \right) + \beta q_{j,k} \\
\frac{\partial e_{i,j}}{\partial b_{i}} &= -2 \left(r_{i,j} - \hat{r}_{i,j}\right) + \beta b_{i} \\
\frac{\partial e_{i,j}}{\partial c_{j}} &= -2 \left(r_{i,j} - \hat{r}_{i,j}\right) + \beta c_{j}
\end{align*}

We can therefore implement a set of update rules as follows:
\begin{align*}
p_{i,k} &\gets p_{i,k} + \alpha \left(2 \left(r_{i,j} - \hat{r}_{i,j}\right)  q_{j,k} - \beta p_{i,k} \right) \\
q_{j,k} &\gets p_{i,k} + \alpha \left(2 \left(r_{i,j} - \hat{r}_{i,j}\right)  p_{i,k} - \beta q_{j,k} \right) \\
b_{i}   &\gets p_{i,k} + \alpha \left(2 \left(r_{i,j} - \hat{r}_{i,j}\right)          + \beta b_{i}   \right) \\
c_{j}   &\gets p_{i,k} + \alpha \left(2 \left(r_{i,j} - \hat{r}_{i,j}\right)          + \beta c_{j}   \right)
\end{align*}

We update $\vec{p_i}$, $\vec{q_j}$, $b{i}$, and $c_{j}$ for each pair $(i, j, r_{i,j}) \in T$. This process is repeated for some number of steps, or \emph{epochs}, until either some criterion for epochs is met (e.g. the change in the total error $E$ is less than some $\epsilon$, or the maximum number of epochs is exceeded). 

\subsection{Optimization}
The main issue we had with the PMF approach was that there were lots of meta-parameters to tune: the learning rate $\alpha$, the regularization parameter $\beta$, the number of latent factors $K$, and the criteria for determining convergence. We estimated appropriate starting values for $K = 5$ and $\beta = 0.02$ based on literature on the Netflix prize \cite{Mnih:2007wg,Ott:2008tu}, and we experimented to determine that $\alpha = 0.001$ generally produced convergent solutions. We tried the following reductionist methods for validating our approach, and for further tuning these parameters:

\begin{enumerate}
\item Checking the gradient. We verified that our gradient function was correct by finite differences.
\item Recapitulating the user mean results. We tried setting $K=1$, $\beta=0$ and fixing $q_{j,k} = 1$. This gave us an RMSE of 0.77857---slightly better than the user mean results.
\item Reconstruction of fake data. We generated fake values for $\mat{P}$ and $\mat{Q}$, and multiplied them together to get a fake $\mat{R}$. We then trained the model on $T$ containing some fraction of the $r_{i,j}$'s.  
\item Cross-validation. We withheld a subset (between 1--10\%) of our training data, trained the model on the remainder, computed the difference between the predicted ratings and the withheld ratings. We used this method to attempt to set our parameters
 
\end{enumerate}

\subsection{Results and Conclusion}


\bibliography{library}
\bibliographystyle{ieeetr}


\end{document}  
